{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     /Users/shihaozhang/nltk_data...\n",
      "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMPLE SUPPLIES LIMIT U.S. STRIKE'S OIL PRICE IMPACT\n",
      "  Ample supplies of OPEC crude weighing on\n",
      "  world markets helped limit and then reverse oil price gains\n",
      "  that followed the U.S. Strike on an Iranian oil platform in the\n",
      "  Gulf earlier on Monday, analysts said.\n",
      "      December loading rose to 19.65 dlrs, up 45 cents before\n",
      "  falling to around 19.05/15 later, unchanged from last Friday.\n",
      "      \"Fundamentals are awful,\" said Philip Lambert, analyst with\n",
      "  stockbrokers Kleinwort Grieveson, adding that total OPEC\n",
      "  production in the first week of October could be above 18.5 mln\n",
      "  bpd, little changed from September levels.\n",
      "      Peter Nicol, analyst at Chase Manhattan Bank, said OPEC\n",
      "  production could be about 18.5-19.0 mln in October. Reuter and\n",
      "  International Energy Agency (IEA) estimates put OPEC September\n",
      "  production at 18.5 mln bpd.\n",
      "      The U.S. Attack was in retaliation of last Friday's hit of\n",
      "  a Kuwaiti oil products tanker flying the U.S. Flag, the Sea\n",
      "  Isle City. It was struck by a missile, believed to be Iranian,\n",
      "  in Kuwai\n"
     ]
    }
   ],
   "source": [
    "tmp = reuters.raw('test/21131')[:1050]\n",
    "print(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMPLE SUPPLIES LIMIT U.S. STRIKE'S OIL PRICE IMPACT\n",
      "  Ample supplies of OPEC crude weighing on\n",
      "  world markets helped limit and then reverse oil price gains\n",
      "  that followed the U.S. Strike on an Iranian oil platform in the\n",
      "  Gulf earlier on Monday, analysts said.\n",
      "\n",
      "\n",
      "December loading rose to 19.65 dlrs, up 45 cents before\n",
      "  falling to around 19.05/15 later, unchanged from last Friday.\n",
      "\n",
      "\n",
      "\"Fundamentals are awful,\" said Philip Lambert, analyst with\n",
      "  stockbrokers Kleinwort Grieveson, adding that total OPEC\n",
      "  production in the first week of October could be above 18.5 mln\n",
      "  bpd, little changed from September levels.\n",
      "\n",
      "\n",
      "Peter Nicol, analyst at Chase Manhattan Bank, said OPEC\n",
      "  production could be about 18.5-19.0 mln in October.\n",
      "\n",
      "\n",
      "Reuter and\n",
      "  International Energy Agency (IEA) estimates put OPEC September\n",
      "  production at 18.5 mln bpd.\n",
      "\n",
      "\n",
      "The U.S.\n",
      "\n",
      "\n",
      "Attack was in retaliation of last Friday's hit of\n",
      "  a Kuwaiti oil products tanker flying the U.S.\n",
      "\n",
      "\n",
      "Flag, the Sea\n",
      "  Isle City.\n",
      "\n",
      "\n",
      "It was struck by a missile, believed to be Iranian,\n",
      "  in Kuwai\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(tmp)\n",
    "for idx, sent in enumerate(sentences):\n",
    "    print(sent)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AMPLE', 'SUPPLIES', 'LIMIT', 'U.S.', 'STRIKE', \"'S\", 'OIL', 'PRICE', 'IMPACT', 'Ample', 'supplies', 'of', 'OPEC', 'crude', 'weighing', 'on', 'world', 'markets', 'helped', 'limit', 'and', 'then', 'reverse', 'oil', 'price', 'gains', 'that', 'followed', 'the', 'U.S.', 'Strike', 'on', 'an', 'Iranian', 'oil', 'platform', 'in', 'the', 'Gulf', 'earlier', 'on', 'Monday', ',', 'analysts', 'said', '.', 'December', 'loading', 'rose', 'to', '19.65', 'dlrs', ',', 'up', '45', 'cents', 'before', 'falling', 'to', 'around', '19.05/15', 'later', ',', 'unchanged', 'from', 'last', 'Friday', '.', '``', 'Fundamentals', 'are', 'awful', ',', \"''\", 'said', 'Philip', 'Lambert', ',', 'analyst', 'with', 'stockbrokers', 'Kleinwort', 'Grieveson', ',', 'adding', 'that', 'total', 'OPEC', 'production', 'in', 'the', 'first', 'week', 'of', 'October', 'could', 'be', 'above', '18.5', 'mln', 'bpd', ',', 'little', 'changed', 'from', 'September', 'levels', '.', 'Peter', 'Nicol', ',', 'analyst', 'at', 'Chase', 'Manhattan', 'Bank', ',', 'said', 'OPEC', 'production', 'could', 'be', 'about', '18.5-19.0', 'mln', 'in', 'October', '.', 'Reuter', 'and', 'International', 'Energy', 'Agency', '(', 'IEA', ')', 'estimates', 'put', 'OPEC', 'September', 'production', 'at', '18.5', 'mln', 'bpd', '.', 'The', 'U.S', '.', 'Attack', 'was', 'in', 'retaliation', 'of', 'last', 'Friday', \"'s\", 'hit', 'of', 'a', 'Kuwaiti', 'oil', 'products', 'tanker', 'flying', 'the', 'U.S', '.', 'Flag', ',', 'the', 'Sea', 'Isle', 'City', '.', 'It', 'was', 'struck', 'by', 'a', 'missile', ',', 'believed', 'to', 'be', 'Iranian', ',', 'in', 'Kuwai']\n"
     ]
    }
   ],
   "source": [
    "print(nltk.word_tokenize(tmp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- mostly a rule-based tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1720901"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nltk.corpus.reuters.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('.', 94687), (',', 72360), ('the', 58251), ('of', 35979), ('to', 34035), ('in', 26478), ('said', 25224), ('and', 25043), ('a', 23492), ('mln', 18037)]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "fdist = nltk.FreqDist(nltk.corpus.reuters.words())\n",
    "# top 10 most frequent words\n",
    "print(fdist.most_common(n=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2346\n",
      "0\n",
      "0.033849129031826936\n",
      "['RIFT', 'Mounting', 'inflict', 'Move', 'Unofficial', 'Sheen', 'Safe', 'avowed', 'VERMIN', 'EAT'] ...\n",
      "41600\n",
      "1720901\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# get the count of the word `stock` \n",
    "print(fdist['stock']) # 2346\n",
    "# get the count of the word `stork` \n",
    "print(fdist['stork']) # 0 :(\n",
    "# get the frequency of the word `the` \n",
    "print(fdist.freq('the')) # 0.033849129031826936\n",
    "\n",
    "# get the words that only appear once (these words are called hapaxes)\n",
    "print(fdist.hapaxes()[:10], '...')\n",
    "# Hapaxes usually are `mispeled` or weirdly `cApiTALIZED` words.\n",
    "\n",
    "# Total number of distinct words \n",
    "print(len(fdist.keys())) # 41600\n",
    "# Total number of samples \n",
    "print(fdist.N()) # 1720901\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bigrams & 3grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('John', 'works'), ('works', 'at'), ('at', 'Intel'), ('Intel', '.')]\n",
      "[('John', 'works', 'at'), ('works', 'at', 'Intel'), ('at', 'Intel', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import bigrams, trigrams, word_tokenize \n",
    "text = \"John works at Intel.\"\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "print(list(bigrams(tokens))) # the `bigrams` function returns a generator, so we must unwind it\n",
    "\n",
    "print(list(trigrams(tokens))) # the `trigrams` function returns a generator, so we must unwind it # [('John', 'works', 'at'), ('works', 'at', 'Intel'), ('at', 'Intel', '.')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### collocation\n",
    "- important collocation features for 2/3grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder \n",
    "from nltk.collocations import TrigramAssocMeasures, TrigramCollocationFinder\n",
    "\n",
    "bigram_measures = BigramAssocMeasures() \n",
    "trigram_measures = TrigramAssocMeasures()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "finder = BigramCollocationFinder.from_words(nltk.corpus.reuters.words()) \n",
    "\n",
    "# only bigrams that appear 5+ times\n",
    "finder.apply_freq_filter(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('DU', 'PONT'), ('Keng', 'Yaik'), ('Kwik', 'Save'), ('Nihon', 'Keizai'), ('corenes', 'pora'), ('fluidized', 'bed'), ('Akbar', 'Hashemi'), ('Constructions', 'Telephoniques'), ('Elevator', 'Mij'), ('Entre', 'Rios'), ('Graan', 'Elevator'), ('JIM', 'WALTER'), ('Taikoo', 'Shing'), ('der', 'Vorm'), ('di', 'Clemente'), ('Borrowing', 'Requirement'), ('FOOTE', 'MINERAL'), ('Hawker', 'Siddeley'), ('JARDINE', 'MATHESON'), ('PRORATION', 'FACTOR'), ('Wildlife', 'Refuge'), ('Kohlberg', 'Kravis'), ('Almir', 'Pazzionotto'), ('Bankhaus', 'Centrale'), ('Corpus', 'Christi'), ('Kuala', 'Lumpur'), ('Maple', 'Leaf'), ('Stats', 'Oljeselskap'), ('Zoete', 'Wedd'), ('Tadashi', 'Kuranari'), ('Drawing', 'Rights'), ('EASTMAN', 'KODAK'), ('Martinez', 'Cuenca'), ('Mathematical', 'Applications'), ('Neutral', 'Zone'), ('Townsend', 'Thoresen'), ('Sector', 'Borrowing'), ('Hashemi', 'Rafsanjani'), ('Hossein', 'Mousavi'), ('Kitty', 'Hawk'), ('Task', 'Force'), ('Tender', 'Loving'), ('WELLS', 'FARGO'), ('SLAUGHTER', 'GUESSTIMATES'), ('ad', 'hoc'), ('mechanically', 'separated'), ('bleached', 'deodorised'), ('Alejandro', 'Martinez'), ('Salina', 'Cruz'), ('Het', 'Comite')]\n"
     ]
    }
   ],
   "source": [
    "# return the 50 bigrams with the highest PMI (Pointwise Mutual Information)\n",
    "print(finder.nbest(bigram_measures.pmi, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Graan', 'Elevator', 'Mij'), ('Sector', 'Borrowing', 'Requirement'), ('Akbar', 'Hashemi', 'Rafsanjani'), ('Lim', 'Keng', 'Yaik'), ('Alejandro', 'Martinez', 'Cuenca'), ('Den', 'Norske', 'Stats'), ('Norske', 'Stats', 'Oljeselskap'), ('Kokusai', 'Denshin', 'Denwa'), ('Special', 'Drawing', 'Rights'), ('Dar', 'es', 'Salaam'), ('FOLLOWING', 'RAINFALL', 'WAS'), ('Duffour', 'et', 'Igon'), ('Tender', 'Loving', 'Care'), ('CATTLE', 'SLAUGHTER', 'GUESSTIMATES'), ('CAMPBELL', 'RED', 'LAKE'), ('Victor', 'Paz', 'Estenssoro'), ('Carter', 'Hawley', 'Hale'), ('Punta', 'del', 'Este'), ('ELEVATOR', 'LOADING', 'WAITING'), ('TIME', 'JOBLESS', 'CLAIMS'), ('Francaise', 'des', 'Petroles'), ('Public', 'Sector', 'Borrowing'), ('Arturo', 'Hernandez', 'Grisanti'), ('Speaker', 'Jim', 'Wright'), ('carrier', 'Kitty', 'Hawk'), ('Archer', 'Daniels', 'Midland'), ('Corning', 'Glass', 'Works'), ('refined', 'bleached', 'deodorised'), ('Grown', 'Cereals', 'Authority'), ('Commissioner', 'Frans', 'Andriessen'), ('RBD', 'PALM', 'OLEIN'), ('RAINFALL', 'THE', 'FOLLOWING'), ('THE', 'FOLLOWING', 'RAINFALL'), ('Kremlin', 'leader', 'Mikhail'), ('Bankhaus', 'Centrale', 'Credit'), ('SANTA', 'FE', 'SOUTHERN'), ('Director', 'Kobena', 'Erbynn'), ('THOUS', 'BUSHELS', 'SOYBEANS'), ('GETS', 'QUALIFIED', 'AUDIT'), ('Denis', 'Bra', 'Kanon'), ('GHANA', 'COCOA', 'PURCHASES'), ('bleached', 'deodorised', 'palm'), ('leader', 'Mikhail', 'Gorbachev'), ('SLAUGHTER', 'GUESSTIMATES', 'Chicago'), ('de', 'Constructions', 'Telephoniques'), ('DISCOUNT', 'WINDOW', 'BORROWINGS'), ('Nil', 'Nil', 'Nil'), ('Fichtel', 'und', 'Sachs'), ('de', 'Zoete', 'Wedd'), ('Home', 'Grown', 'Cereals')]\n"
     ]
    }
   ],
   "source": [
    "# among the collocations we can find stuff like: (u'Corpus', u'Christi') ...\n",
    "# Compute length-3 collocations\n",
    "finder = nltk.collocations.TrigramCollocationFinder.from_words(nltk.corpus.reuters.words()) # only trigrams that appear 5+ times\n",
    "finder.apply_freq_filter(5)\n",
    "# return the 50 trigrams with the highest PMI\n",
    "print(finder.nbest(trigram_measures.pmi, 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:  ['Things', 'I', 'wish', 'I', 'knew', 'before', 'I', 'started', 'blogging', '.']\n",
      "Tagged Tokens:  [('Things', 'NNS'), ('I', 'PRP'), ('wish', 'VBP'), ('I', 'PRP'), ('knew', 'VBD'), ('before', 'IN'), ('I', 'PRP'), ('started', 'VBD'), ('blogging', 'VBG'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Things I wish I knew before I started blogging.\" \n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "print(\"Tokens: \", tokens)\n",
    "\n",
    "tagged_tokens = nltk.pos_tag(tokens) \n",
    "print(\"Tagged Tokens: \", tagged_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The closing chapter, is adapted from the address that Feynman gave during the 1974 commencement exercises\n",
      "at the California Institute Of Technology. \n"
     ]
    }
   ],
   "source": [
    "sentence = \"\"\"The closing chapter, is adapted from the address that Feynman gave during the 1974 commencement exercises\n",
    "at the California Institute Of Technology. \"\"\"\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(sentence)\n",
    "tagged_tokens = nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_tree = nltk.ne_chunk(tagged_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  The/DT\n",
      "  closing/NN\n",
      "  chapter/NN\n",
      "  ,/,\n",
      "  is/VBZ\n",
      "  adapted/VBN\n",
      "  from/IN\n",
      "  the/DT\n",
      "  address/NN\n",
      "  that/IN\n",
      "  (PERSON Feynman/NNP)\n",
      "  gave/VBD\n",
      "  during/IN\n",
      "  the/DT\n",
      "  1974/CD\n",
      "  commencement/NN\n",
      "  exercises/NNS\n",
      "  at/IN\n",
      "  the/DT\n",
      "  (ORGANIZATION California/NNP Institute/NNP Of/IN Technology/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "print(ner_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- an NLTK tree for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"\"\"Jim goes to Harvard.\"\"\"\n",
    "tokens = nltk.word_tokenize(sentence) \n",
    "tagged_tokens = nltk.pos_tag(tokens) \n",
    "ner_annotated_tree = nltk.ne_chunk(tagged_tokens) \n",
    "ner_annotated_tree.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- large english lexical database created @Princeton\n",
    "- important structural relationship includes\n",
    "  - synonym\n",
    "  - hyponym / hypernymy (more specific/broader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['car', 'auto', 'automobile', 'machine', 'motorcar']\n",
      "a motor vehicle with four wheels; usually propelled by an internal combustion engine\n",
      "\n",
      "['car', 'railcar', 'railway_car', 'railroad_car']\n",
      "a wheeled vehicle adapted to the rails of railroad\n",
      "\n",
      "['car', 'gondola']\n",
      "the compartment that is suspended from an airship and that carries personnel and the cargo and the power plant\n",
      "\n",
      "['car', 'elevator_car']\n",
      "where passengers ride up and down\n",
      "\n",
      "['cable_car', 'car']\n",
      "a conveyance for passengers or freight on a cable railway\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for car in wn.synsets('car'):\n",
    "    print([l.name() for l in car.lemmas()]) \n",
    "    print(car.definition())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle = wn.synsets('vehicle')[0]\n",
    "t = nltk.Tree(\n",
    "    vehicle.name(), \n",
    "    children=[ \n",
    "        nltk.Tree(vehicle.hyponyms()[3].name(), children=[]), \n",
    "        nltk.Tree(vehicle.hyponyms()[4].name(), children=[]), \n",
    "        nltk.Tree(vehicle.hyponyms()[5].name(), children=[]), \n",
    "        nltk.Tree(vehicle.hyponyms()[7].name(), children=[\n",
    "            nltk.Tree(vehicle.hyponyms()[7].hyponyms()[1].name(), children=[]\n",
    "         ), \n",
    "        nltk.Tree(vehicle.hyponyms()[7].hyponyms()[3].name(), children=[]),\n",
    "        nltk.Tree(vehicle.hyponyms()[7].hyponyms()[4].name(), children=[]), \n",
    "            nltk.Tree(vehicle.hyponyms()[7].hyponyms()[5].name(), children=[]), \n",
    "            nltk.Tree(vehicle.hyponyms()[7].hyponyms()[6].name(), children=[]),\n",
    "]), ])\n",
    "t.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Synset('bumper_car.n.01'),\n",
       "  'a small low-powered electrically powered vehicle driven on a special platform where there are many others to be dodged'),\n",
       " (Synset('craft.n.02'),\n",
       "  'a vehicle designed for navigation in or on water or air or through outer space'),\n",
       " (Synset('military_vehicle.n.01'), 'vehicle used by the armed forces'),\n",
       " (Synset('rocket.n.01'), 'any vehicle self-propelled by a rocket engine'),\n",
       " (Synset('skibob.n.01'),\n",
       "  'a vehicle resembling a bicycle but having skis instead of wheels; the rider wears short skis for balancing'),\n",
       " (Synset('sled.n.01'),\n",
       "  'a vehicle mounted on runners and pulled by horses or dogs; for transportation over snow'),\n",
       " (Synset('steamroller.n.02'),\n",
       "  'vehicle equipped with heavy wide smooth rollers for compacting roads and pavements'),\n",
       " (Synset('wheeled_vehicle.n.01'),\n",
       "  'a vehicle that moves on wheels and usually has a container for transporting things or people')]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(x, x.definition()) for x in vehicle.hyponyms()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('walk.v.01')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## querying a specific POS\n",
    "walk = wn.synset('walk.v.01')\n",
    "walk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wordnet Lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'talk': 108, 'speak': 53}\n"
     ]
    }
   ],
   "source": [
    "talk = wn.synset('talk.v.01')\n",
    "print({lemma.name(): lemma.count() for lemma in talk.lemmas()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lemma('ugly.a.01.ugly')]\n"
     ]
    }
   ],
   "source": [
    "# antonyms = opposite\n",
    "beautiful = wn.synset('beautiful.a.01') \n",
    "print(beautiful.lemmas()[0].antonyms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lemma('ability.n.02.ability'), Lemma('ability.n.01.ability')]\n"
     ]
    }
   ],
   "source": [
    "able = wn.synset('able.a.01') \n",
    "print(able.lemmas()[0].derivationally_related_forms())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatizing + Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Stemmers can’t guarantee a valid word as a result (in fact, the result usually isn’t a valid word)\n",
    "- Lemmatizers always return a valid word (the dictionary form)\n",
    "- Stemmers are faster\n",
    "- Lemmatizers depend on a corpus (basically a dictionary) to perform the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "friend\n",
      "friend\n",
      "friend\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "# all resolve to `friend`\n",
    "print(stemmer.stem('friend')) \n",
    "print(stemmer.stem('friends')) \n",
    "print(stemmer.stem('friendly'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drink\n",
      "drunk\n",
      "slowli\n",
      "xyze\n"
     ]
    }
   ],
   "source": [
    "# Not working well with irregulars \n",
    "print(stemmer.stem('drink')) # `drink` \n",
    "print(stemmer.stem('drunk')) # `drunk`\n",
    "\n",
    "# Not a proper word (`slowli`)\n",
    "print(stemmer.stem('slowly'))\n",
    "\n",
    "# Works with non-existing words (`xyze`)\n",
    "print(stemmer.stem('xyzing'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "haunting\n",
      "haunt\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize('haunting', 'n')) # `haunting` \n",
    "print(lemmatizer.lemmatize('haunting', 'v')) # `haunt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "friend\n",
      "friend\n",
      "friendly\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize('friend', 'n')) \n",
    "print(lemmatizer.lemmatize('friends', 'n'))\n",
    "\n",
    "print(lemmatizer.lemmatize('friendly', 'a'))  # as adjective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drink\n",
      "drink\n"
     ]
    }
   ],
   "source": [
    "# work w/ irregular\n",
    "print(lemmatizer.lemmatize('drink', 'v')) # `drink` \n",
    "print(lemmatizer.lemmatize('drunk', 'v')) # `drink`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xyzing\n"
     ]
    }
   ],
   "source": [
    "# only work w/ seen word\n",
    "print(lemmatizer.lemmatize('xyzing', 'v'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
