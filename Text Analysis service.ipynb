{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text service\n",
    "\n",
    "one of the most popular task is text classification.\n",
    "\n",
    "popular examples like\n",
    "- domain classification\n",
    "- sentiment analysis (sentiment label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## example: name - sex classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from nltk.corpus import names\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "girl_names = names.words('female.txt')\n",
    "boy_names = names.words('male.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Abagael',\n",
       " 'Abagail',\n",
       " 'Abbe',\n",
       " 'Abbey',\n",
       " 'Abbi',\n",
       " 'Abbie',\n",
       " 'Abby',\n",
       " 'Abigael',\n",
       " 'Abigail',\n",
       " 'Abigale']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "girl_names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2943"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(boy_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1773"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "girl_names_ending_in_a = [name for name in girl_names if name.endswith('a')]\n",
    "len(girl_names_ending_in_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "girl_names_ending_letters = collections.Counter([name[-1] for name in girl_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1773),\n",
       " ('e', 1432),\n",
       " ('y', 461),\n",
       " ('n', 386),\n",
       " ('i', 317),\n",
       " ('l', 179),\n",
       " ('h', 105),\n",
       " ('s', 93),\n",
       " ('t', 68),\n",
       " ('r', 47),\n",
       " ('d', 39),\n",
       " ('o', 33),\n",
       " ('m', 13),\n",
       " ('g', 10),\n",
       " ('x', 10),\n",
       " ('b', 9),\n",
       " ('u', 6),\n",
       " ('w', 5),\n",
       " ('z', 4),\n",
       " ('k', 3),\n",
       " ('v', 2),\n",
       " ('p', 2),\n",
       " ('f', 2),\n",
       " (' ', 1),\n",
       " ('j', 1)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "girl_names_ending_letters.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('n', 478),\n",
       " ('e', 468),\n",
       " ('y', 332),\n",
       " ('s', 230),\n",
       " ('d', 228),\n",
       " ('r', 190),\n",
       " ('l', 187),\n",
       " ('o', 165),\n",
       " ('t', 164),\n",
       " ('h', 93),\n",
       " ('m', 70),\n",
       " ('k', 69),\n",
       " ('i', 50),\n",
       " ('g', 32),\n",
       " ('a', 29),\n",
       " ('f', 25),\n",
       " ('c', 25),\n",
       " ('b', 21),\n",
       " ('p', 18),\n",
       " ('w', 17),\n",
       " ('v', 16),\n",
       " ('u', 12),\n",
       " ('z', 11),\n",
       " ('x', 10),\n",
       " ('j', 3)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boy_names_ending_letters = collections.Counter([name[-1] for name in boy_names])\n",
    "boy_names_ending_letters.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(name: str):\n",
    "    return {\n",
    "        'last_letter': name[-1]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "boy_names_data = [(extract_features(name), 'boy') for name in boy_names]\n",
    "girl_names_data = [(extract_features(name), 'girl') for name in girl_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_agg = boy_names_data + girl_names_data\n",
    "random.shuffle(data_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = int(0.75 * len(data_agg))\n",
    "train, test = data_agg[:cutoff], data_agg[cutoff+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision tree\n",
    "name_tree = nltk.DecisionTreeClassifier.train(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "David: boy\n",
      "Alex: girl\n",
      "Alexander: boy\n",
      "Alexa: girl\n",
      "Becca: girl\n",
      "Rosy: girl\n"
     ]
    }
   ],
   "source": [
    "examples = ['David', 'Alex', 'Alexander', 'Alexa', 'Becca', 'Rosy']\n",
    "for x in examples:\n",
    "    print('{}: {}'.format(x, name_tree.classify(extract_features(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7682619647355163\n"
     ]
    }
   ],
   "source": [
    "# test accuracy\n",
    "print(nltk.classify.accuracy(name_tree, test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_letter= ? ........................................ girl\n",
      "last_letter=a? ........................................ girl\n",
      "last_letter=b? ........................................ boy\n",
      "last_letter=c? ........................................ boy\n",
      "last_letter=d? ........................................ boy\n",
      "last_letter=e? ........................................ girl\n",
      "last_letter=f? ........................................ boy\n",
      "last_letter=g? ........................................ boy\n",
      "last_letter=h? ........................................ girl\n",
      "last_letter=i? ........................................ girl\n",
      "last_letter=j? ........................................ boy\n",
      "last_letter=k? ........................................ boy\n",
      "last_letter=l? ........................................ boy\n",
      "last_letter=m? ........................................ boy\n",
      "last_letter=n? ........................................ boy\n",
      "last_letter=o? ........................................ boy\n",
      "last_letter=p? ........................................ boy\n",
      "last_letter=r? ........................................ boy\n",
      "last_letter=s? ........................................ boy\n",
      "last_letter=t? ........................................ boy\n",
      "last_letter=u? ........................................ boy\n",
      "last_letter=v? ........................................ boy\n",
      "last_letter=w? ........................................ boy\n",
      "last_letter=x? ........................................ girl\n",
      "last_letter=y? ........................................ girl\n",
      "last_letter=z? ........................................ boy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tree\n",
    "print(name_tree.pretty_format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def extract_features2(name: str):\n",
    "    features = {\n",
    "        'last_letter': name[-1],\n",
    "        'vowel_count': len([c for c in name if c.lower() in 'aeiou']),\n",
    "        'first_letter': name[0],\n",
    "    }\n",
    "    for c in string.ascii_lowercase:\n",
    "        features['contains_' + c] = c in name\n",
    "        features['count_' + c] = name.lower().count(c) \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep_split_train(func_feature, boy_names, girl_names):\n",
    "    data = [(func_feature(name), 'boy') for name in boy_names] + [(func_feature(name), 'girl') for name in girl_names]\n",
    "    random.shuffle(data)\n",
    "    cutoff = int(0.75 * len(data))\n",
    "    train, test = data[:cutoff], data[cutoff+1:]\n",
    "    return nltk.DecisionTreeClassifier.train(train), train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_tree2, train, test = data_prep_split_train(extract_features2, boy_names, girl_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7803526448362721"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(name_tree2, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_letter= ? ........................................ girl\n",
      "last_letter=a? ........................................ girl\n",
      "last_letter=b? ........................................ boy\n",
      "  vowel_count=2? ...................................... boy\n",
      "  vowel_count=0? ...................................... girl\n",
      "  vowel_count=1? ...................................... boy\n",
      "last_letter=c? ........................................ boy\n",
      "last_letter=d? ........................................ boy\n",
      "last_letter=e? ........................................ girl\n",
      "  vowel_count=2? ...................................... girl\n",
      "  vowel_count=3? ...................................... girl\n",
      "  vowel_count=4? ...................................... girl\n",
      "  vowel_count=5? ...................................... girl\n",
      "  vowel_count=6? ...................................... girl\n",
      "  vowel_count=1? ...................................... boy\n",
      "last_letter=f? ........................................ boy\n",
      "last_letter=g? ........................................ boy\n",
      "last_letter=h? ........................................ girl\n",
      "  vowel_count=2? ...................................... girl\n",
      "  vowel_count=3? ...................................... girl\n",
      "  vowel_count=4? ...................................... boy\n",
      "  vowel_count=6? ...................................... girl\n",
      "  vowel_count=1? ...................................... boy\n",
      "last_letter=i? ........................................ girl\n",
      "  vowel_count=2? ...................................... girl\n",
      "  vowel_count=3? ...................................... girl\n",
      "  vowel_count=4? ...................................... boy\n",
      "  vowel_count=5? ...................................... boy\n",
      "  vowel_count=1? ...................................... girl\n",
      "last_letter=j? ........................................ boy\n",
      "last_letter=k? ........................................ boy\n",
      "last_letter=l? ........................................ girl\n",
      "  vowel_count=2? ...................................... boy\n",
      "  vowel_count=3? ...................................... girl\n",
      "  vowel_count=4? ...................................... boy\n",
      "  vowel_count=0? ...................................... girl\n",
      "  vowel_count=1? ...................................... girl\n",
      "last_letter=m? ........................................ boy\n",
      "  vowel_count=2? ...................................... boy\n",
      "  vowel_count=3? ...................................... boy\n",
      "  vowel_count=4? ...................................... boy\n",
      "  vowel_count=0? ...................................... girl\n",
      "  vowel_count=1? ...................................... boy\n",
      "last_letter=n? ........................................ girl\n",
      "  vowel_count=2? ...................................... boy\n",
      "  vowel_count=3? ...................................... girl\n",
      "  vowel_count=4? ...................................... girl\n",
      "  vowel_count=5? ...................................... boy\n",
      "  vowel_count=0? ...................................... girl\n",
      "  vowel_count=1? ...................................... girl\n",
      "last_letter=o? ........................................ boy\n",
      "last_letter=p? ........................................ boy\n",
      "last_letter=r? ........................................ girl\n",
      "  vowel_count=2? ...................................... boy\n",
      "  vowel_count=3? ...................................... boy\n",
      "  vowel_count=4? ...................................... boy\n",
      "  vowel_count=1? ...................................... girl\n",
      "last_letter=s? ........................................ girl\n",
      "  vowel_count=2? ...................................... boy\n",
      "  vowel_count=3? ...................................... boy\n",
      "  vowel_count=4? ...................................... boy\n",
      "  vowel_count=5? ...................................... boy\n",
      "  vowel_count=0? ...................................... girl\n",
      "  vowel_count=1? ...................................... boy\n",
      "last_letter=t? ........................................ boy\n",
      "last_letter=u? ........................................ boy\n",
      "last_letter=v? ........................................ boy\n",
      "last_letter=w? ........................................ boy\n",
      "last_letter=x? ........................................ girl\n",
      "  vowel_count=2? ...................................... girl\n",
      "  vowel_count=3? ...................................... girl\n",
      "  vowel_count=1? ...................................... boy\n",
      "last_letter=y? ........................................ boy\n",
      "  vowel_count=2? ...................................... girl\n",
      "  vowel_count=3? ...................................... girl\n",
      "  vowel_count=4? ...................................... girl\n",
      "  vowel_count=5? ...................................... girl\n",
      "  vowel_count=0? ...................................... boy\n",
      "  vowel_count=1? ...................................... girl\n",
      "last_letter=z? ........................................ boy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tree\n",
    "print(name_tree2.pretty_format())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-learn ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import names\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def extract_features(name): \n",
    "    \"\"\"Get the features used for name classification \"\"\"\n",
    "    return {\n",
    "        'last_letter': name[-1] \n",
    "    }\n",
    "\n",
    "def extract_features_big(name: str):\n",
    "    features = {\n",
    "        'last_letter': name[-1],\n",
    "        'vowel_count': len([c for c in name if c.lower() in 'aeiou']),\n",
    "        'first_letter': name[0],\n",
    "    }\n",
    "    for c in string.ascii_lowercase:\n",
    "        features['contains_' + c] = c in name\n",
    "        features['count_' + c] = name.lower().count(c) \n",
    "    return features\n",
    "\n",
    "# Get the names\n",
    "boy_names = names.words('male.txt') \n",
    "girl_names = names.words('female.txt')\n",
    "\n",
    "# Build the dataset\n",
    "boy_names_dataset = [(extract_features_big(name), 'boy') for name in boy_names] \n",
    "girl_names_dataset = [(extract_features_big(name), 'girl') for name in girl_names]\n",
    "\n",
    "# Put all the names together\n",
    "data = boy_names_dataset + girl_names_dataset \n",
    "\n",
    "# Split the data in features and classes\n",
    "X, y = list(zip(*data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split and randomize\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25, shuffle=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tree in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "dict_vec = DictVectorizer()\n",
    "name_tree = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_vec.fit(X_train)\n",
    "X_train_vectorized = dict_vec.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_tree.fit(X_train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9652567975830816"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_tree.score(X_train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7472306143001007"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_vectorized = dict_vec.transform(X_test)\n",
    "name_tree.score(X_test_vectorized, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## try on existing corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['acq', 'alum', 'barley', 'bop', 'carcass', 'castor-oil', 'cocoa', 'coconut', 'coconut-oil', 'coffee', 'copper', 'copra-cake', 'corn', 'cotton', 'cotton-oil', 'cpi', 'cpu', 'crude', 'dfl', 'dlr', 'dmk', 'earn', 'fuel', 'gas', 'gnp', 'gold', 'grain', 'groundnut', 'groundnut-oil', 'heat', 'hog', 'housing', 'income', 'instal-debt', 'interest', 'ipi', 'iron-steel', 'jet', 'jobs', 'l-cattle', 'lead', 'lei', 'lin-oil', 'livestock', 'lumber', 'meal-feed', 'money-fx', 'money-supply', 'naphtha', 'nat-gas', 'nickel', 'nkr', 'nzdlr', 'oat', 'oilseed', 'orange', 'palladium', 'palm-oil', 'palmkernel', 'pet-chem', 'platinum', 'potato', 'propane', 'rand', 'rape-oil', 'rapeseed', 'reserves', 'retail', 'rice', 'rubber', 'rye', 'ship', 'silver', 'sorghum', 'soy-meal', 'soy-oil', 'soybean', 'strategic-metal', 'sugar', 'sun-meal', 'sun-oil', 'sunseed', 'tea', 'tin', 'trade', 'veg-oil', 'wheat', 'wpi', 'yen', 'zinc']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import reuters\n",
    "print(reuters.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shihaozhang/Desktop/ML_tech/nlp_for_hackers/nlp4hackers/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "news20 = fetch_20newsgroups(subset='train') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news20.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build sample dataset\n",
    "- download data from pocket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORIES = { 'business': [\n",
    "        \"Business\",\n",
    "        \"Marketing\",\n",
    "        \"Management\"\n",
    "    ],\n",
    "'family': [\n",
    "        \"Family\",\n",
    "        \"Children\",\n",
    "        \"Parenting\"\n",
    "    ], 'politics': [\n",
    "        \"Politics\",\n",
    "        \"Presidential Elections\",\n",
    "        \"Politicians\",\n",
    "        \"Government\",\n",
    "        \"Congress\"\n",
    "    ],\n",
    "'sport': [\n",
    "        \"Baseball\",\n",
    "        \"Basketball\",\n",
    "        \"Running\",\n",
    "        \"Sport\",\n",
    "        \"Skiing\",\n",
    "        \"Gymnastics\",\n",
    "        \"Tenis\",\n",
    "        \"Football\",\n",
    "        \"Soccer\"\n",
    "    ], 'health': [\n",
    "        \"Health\",\n",
    "        \"Weightloss\",\n",
    "        \"Wellness\",\n",
    "        \"Well being\",\n",
    "        \"Vitamins\",\n",
    "        \"Healthy Food\",\n",
    "        \"Healthy Diet\"\n",
    "    ],\n",
    "'economics': [\n",
    "        \"Economics\",\n",
    "        \"Finance\",\n",
    "        \"Accounting\"\n",
    "    ], 'celebrities': [\n",
    "        \"Celebrities\",\n",
    "        \"Showbiz\"\n",
    "    ],\n",
    "'medical': [\n",
    "        \"Medicine\",\n",
    "        \"Doctors\",\n",
    "        \"Health System\",\n",
    "        \"Surgery\",\n",
    "        \"Genetics\",\n",
    "        \"Hospital\"\n",
    "    ],\n",
    "'science & technology': [\n",
    "        \"Galaxy\",\n",
    "        \"Physics\",\n",
    "        \"Technology\",\n",
    "        \"Science\"\n",
    "    ],\n",
    "'information technology': [\n",
    "        \"Artificial Intelligence\",\n",
    "        \"Search Engine\",\n",
    "        \"Software\",\n",
    "        \"Hardware\",\n",
    "        \"Big Data\",\n",
    "        \"Analytics\",\n",
    "        \"Programming\"\n",
    "    ],\n",
    "'education': [\n",
    "        \"Education\",\n",
    "        \"Students\",\n",
    "        \"University\"\n",
    "    ], 'media': [\n",
    "        \"Newspaper\",\n",
    "        \"Reporters\",\n",
    "        \"Social Media\"\n",
    "    ],\n",
    "'cooking': [\n",
    "        \"Cooking\",\n",
    "        \"Gastronomy\",\n",
    "        \"Cooking Recipes\",\n",
    "        \"Paleo Cooking\",\n",
    "        \"Vegan Recipes\"\n",
    "    ], 'religion': [\n",
    "        \"Religion\",\n",
    "        \"Church\",\n",
    "        \"Spirituality\"\n",
    "    ],\n",
    "'legal': [\n",
    "        \"Legal\",\n",
    "        \"Lawyer\",\n",
    "        \"Constitution\"\n",
    "    ], 'history': [\n",
    "        \"Archeology\",\n",
    "        \"History\",\n",
    "        \"Middle Ages\"\n",
    "    ],\n",
    "'nature & ecology': [\n",
    "        \"Nature\",\n",
    "        \"Ecology\",\n",
    "        \"Endangered Species\",\n",
    "        \"Permaculture\"\n",
    "    ], 'travel': [\n",
    "        \"Travel\",\n",
    "        \"Tourism\",\n",
    "        \"Globetrotter\"\n",
    "    ],\n",
    "'meteorology': [\n",
    "        \"Tornado\",\n",
    "        \"Meteorology\",\n",
    "        \"Weather Prediction\"\n",
    "    ], 'automobiles': [\n",
    "        \"Automobiles\",\n",
    "        \"Motorcycles\",\n",
    "        \"Formula 1\",\n",
    "        \"Driving\"\n",
    "    ],\n",
    "'art & traditions': [\n",
    "        \"Art\",\n",
    "        \"Artwork\",\n",
    "        \"Traditions\",\n",
    "        \"Artisan\",\n",
    "        \"Pottery\",\n",
    "        \"Painting\",\n",
    "        \"Artist\"\n",
    "    ],\n",
    "'beauty & fashion': [\n",
    "        \"Beauty\",\n",
    "        \"Fashion\",\n",
    "        \"Cosmetics\",\n",
    "        \"Makeup\"\n",
    "    ],\n",
    "'relationships': [\n",
    "        \"Relationships\",\n",
    "        \"Relationship Advice\",\n",
    "        \"Marriage\",\n",
    "        \"Wedding\"\n",
    "    ], 'astrology': [\n",
    "        \"Astrology\",\n",
    "        \"Zodiac\",\n",
    "        \"Zodiac Signs\",\n",
    "        \"Horoscope\"\n",
    "    ],\n",
    "'diy': [\n",
    "'Gardening', 'Construction', 'Decorating', 'Do it Yourself', 'Furniture'\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import atexit\n",
    "import urllib\n",
    "import random\n",
    "import requests\n",
    "import pandas as pd\n",
    "from time import sleep, time\n",
    "from bs4 import BeautifulSoup\n",
    "from newspaper import Article, ArticleException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "POCKET_BASE_URL = 'https://getpocket.com/explore/%s'\n",
    "df = pd.DataFrame(columns=['title', 'excerpt', 'url', 'file_name', \"keyword\", \"category\"])\n",
    "\n",
    "@atexit.register\n",
    "def save_dataframe():\n",
    "    \"\"\" Before exiting, make sure we save the dataframe to a CSV file \"\"\" \n",
    "    dataframe_name = \"dataframe_{0}.csv\".format(time()) \n",
    "    df.to_csv(dataframe_name, index=False)\n",
    "\n",
    "# shuffle categories\n",
    "categories = list(CATEGORIES.items()) \n",
    "random.shuffle(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://getpocket.com/explore/david'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "POCKET_BASE_URL % urllib.parse.quote_plus('david')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exploring category=legal\n",
      "Indexing article: \"The Lawyer Whose Clients Didn’t Exist\" from \"https://www.theatlantic.com/magazine/archive/2020/05/bp-oil-spill-shrimpers-settlement/609082/\"\n",
      "Indexing article: \"Uber Says Engineer Is on His Own for $180 Million to Google\" from \"https://www.bloomberg.com/news/articles/2020-04-18/uber-says-guilty-engineer-on-his-own-for-180-million-to-google\"\n",
      "Indexing article: \"Lawyers Get Pay Cuts, Furloughs as Firms Grapple With Downturn\" from \"https://www.bloomberg.com/news/articles/2020-04-18/lawyers-get-pay-cuts-furloughs-as-firms-grapple-with-downturn\"\n",
      "Indexing article: \"Nobel laureates condemn 'judicial harassment' of environmental lawyer\" from \"https://www.theguardian.com/world/2020/apr/18/nobel-laureates-condemn-judicial-harassment-of-environmental-lawyer\"\n",
      "Indexing article: \"Covid-19 checkpoints targeting out-of-state residents draw complaints and legal scrutiny\" from \"https://www.washingtonpost.com/local/trafficandcommuting/covid-19-checkpoints-targeting-out-of-state-residents-draw-complaints-and-legal-scrutiny/2020/04/14/3fc0ed42-774e-11ea-b6ff-597f170df8f8_story.html\"\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-128-9011084001fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexcerpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;31m# Need to sleep in order to not get blocked\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for category_name, keywords in categories:\n",
    "    print('exploring category={}'.format(category_name))\n",
    "    for kw in keywords:\n",
    "        result = requests.get(POCKET_BASE_URL % urllib.parse.quote_plus(kw))\n",
    "        soup = BeautifulSoup(result.content, features='html')\n",
    "        media_items = soup.find_all(attrs={\"class\": 'media_item'})\n",
    "        for item_html in media_items:\n",
    "            title_html = item_html.find_all(attrs={'class': 'title'})[0] \n",
    "            title = title_html.text\n",
    "            \n",
    "            url = title_html.a['data-saveurl']\n",
    "            print(\"Indexing article: \\\"{0}\\\" from \\\"{1}\\\"\".format(title, url))\n",
    "            \n",
    "            excerpt = item_html.find_all(attrs={'class': 'excerpt'})[0].text\n",
    "            try:\n",
    "                article = Article(url)\n",
    "                article.download()\n",
    "                article.parse()\n",
    "                content = article.text\n",
    "            except ArticleException as e:\n",
    "                print(\"Encoutered exception when parsing \\\"{0}\\\": \\\"{1}\\\"\".format(url, str(e))) \n",
    "                continue\n",
    "            \n",
    "            if not content:\n",
    "                print(\"Couldn't extract content from \\\"{0}\\\"\".format(url)) \n",
    "                continue\n",
    "                \n",
    "            file_name = \"{0}.txt\".format(str(uuid.uuid4()))\n",
    "            with open('/Users/shihaozhang/Desktop/ML_tech/nlp_for_hackers/{0}'.format(file_name), 'w+') as text_file:\n",
    "                text_file.write(content)\n",
    "\n",
    "            # Append the row in our dataframe\n",
    "            df.loc[len(df)] = [title, excerpt, url, file_name, kw, category_name] \n",
    "            # Need to sleep in order to not get blocked\n",
    "            sleep(random.randint(5, 15))            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- load data from file instead of downloading (took too long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Feature Extractor\n",
    "\n",
    "- bigram / 3gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "How much wood does a woodchuck chuck if a woodchuck could chuck wood\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2g_features = collections.Counter(nltk.bigrams(text.lower().split()))\n",
    "w3g_features = collections.Counter(nltk.trigrams(text.lower().split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t2\n",
      "  (0, 1)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 6)\t2\n",
      "  (0, 7)\t2\n"
     ]
    }
   ],
   "source": [
    "vectorizer.fit([text])\n",
    "print(vectorizer.transform([text]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'how': 3,\n",
       " 'much': 5,\n",
       " 'wood': 6,\n",
       " 'does': 2,\n",
       " 'woodchuck': 7,\n",
       " 'chuck': 0,\n",
       " 'if': 4,\n",
       " 'could': 1}"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix: \n",
      "(2, 8)\n"
     ]
    }
   ],
   "source": [
    "# out of vocab text\n",
    "result = vectorizer.transform([\"Unseen words\", \"BLT sandwich\"]) \n",
    "print(\"matrix: {}\" .format(result))\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Naive Bayes classifier for multinomial models\n",
      "\n",
      "    The multinomial Naive Bayes classifier is suitable for classification with\n",
      "    discrete features (e.g., word counts for text classification). The\n",
      "    multinomial distribution normally requires integer feature counts. However,\n",
      "    in practice, fractional counts such as tf-idf may also work.\n",
      "\n",
      "    Read more in the :ref:`User Guide <multinomial_naive_bayes>`.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pd.set_option('max_colwidth', 100000)\n",
    "\n",
    "print(MultinomialNB.__doc__[:415])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- data collection & train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = '/Users/shihaozhang/Desktop/ML_tech/nlp_for_hackers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('{}/text_analysis_data.csv'.format(base_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'title', 'excerpt', 'url', 'file_name', 'keyword',\n",
       "       'category'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_samples, labels = [], []\n",
    "for idx, row in data.iterrows():\n",
    "    with open('./clean_data/{0}'.format(row['file_name']), 'r') as text_file:\n",
    "        text = text_file.read()\n",
    "        text_samples.append(text)\n",
    "        labels.append(row['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(text_samples, labels, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- featurization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(lowercase=True)\n",
    "vectorizer.fit(X_train)\n",
    "X_train_vectorized = vectorizer.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.811314791403287\n",
      "test accuracy: 0.6449778900821226\n"
     ]
    }
   ],
   "source": [
    "X_test_vectorizaed = vectorizer.transform(X_test)\n",
    "print('train accuracy: {}'.format(model.score(X_train_vectorized, y_train)))\n",
    "print('test accuracy: {}'.format(model.score(X_test_vectorizaed, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classifying using MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random_choice = random.randint(0, len(X_test))\n",
    "text, label = X_test[random_choice], y_test[random_choice]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Humanity has conquered the world. It's hard to appreciate what that means, but the video above, by WorldPopulationHistory.org, shows just how incredible the growth and expansion of humanity has been over the past 2,000 years.\n",
      "\n",
      "Here are some of the notable moments in the video:\n",
      "\n",
      "The map begins at 1:18, showing human population a little more than 2,000 years ago, with each yellow dot representing 1 million people in an area. At this point, there are 170 million people on Earth.\n",
      "\n",
      "At 3:20, the Mongol invasion of China begins in the early 13th century, killing huge segments of the population. The Mongol conquests are still considered one of the deadliest wars in history, killing tens of millions of people at a time when the world population was much smaller — around 360 million.\n",
      "\n",
      "At 3:30, in the 14th century, the Black Death spreads around the world, killing more than 20 million people in Europe — nearly one-third of the continent's population — and 75 million around the world, when the global population was about 380 million.\n",
      "\n",
      "At 4:20, the world population explodes thanks to the Industrial Revolution and modern medicine. From 1800 to 2015, the global population grew from about 910 million to more than 7.3 billion.\n",
      "\n",
      "After 4:40, you can begin to see how the world population will expand in the future — to nearly 9.6 billion.\n",
      "\n",
      "The video itself can get a little dry at points, but what's really impressive is the fully interactive map at WorldPopulationHistory.org. There, you can zoom in, go through different years, and look at significant milestones. Check it out here.\n",
      "\n",
      "VIDEO: 220 years of population shifts in one map \n",
      "------------------------------------\n",
      "label=history\n",
      "Most of us have an opinion one way or another about having children (and for those who aren't sure yet, this post may be especially for you). We asked you the reasons behind your choice, and here are some of the best ones we heard.\n",
      "\n",
      "Because it's such a personal matter, most \"arguments\" were, well, personal. But there were many common themes in the threads.\n",
      "\n",
      "Kids: You Either Love Them and Love Raising Them...\n",
      "\n",
      "Most people who say they decided to become parents, myself included, say they did so because they love kids. Tlkljeff asserts it's the most fulfilling role for him (while supporting others' decisions not to have kids):\n",
      "\n",
      "I've never been particularly career-motivated; my job affords me financial security and I enjoy the work but I'm not passionate about it. I live for the time I get to spend with my wife and young sons. I love sharing my hobbies with them (\"My daddy can fix anything!\") and watching them grow and develop their own interests (our house is filled with trains now). \"I love you, daddy.\" That's it, right there. That's why you do it. Nothing's more important in my life than my family and my role as a dad.\n",
      "\n",
      "As messy as parenting can be, for many it adds a greater purpose to their lives—and untold enjoyment (and ice cream), as JuliaS27 relates:\n",
      "\n",
      "For us, having kids was about dedicating our lives to raising them and loving them instead of just having fun for us. [...] Every morning, I roll over and find one or more children (and the dog) in our bed in various spots, curled around us, flopped over us, and in that moment before my husband starts tickling everyone, I am grateful for the messy, confusing, exhausting, loving, rich life we've built. I love my kids fiercely, and they have taught me a great deal about life. At 8 & 10 they are ridiculously insightful little sponges and while I have no idea what a Mixel is, they keep me centered on something bigger than myself, even if it is just the joy of reading a Mad Magazine on a summer night and eating s'mores. Plus you have the perfect excuse to eat ice cream almost all of the time and play with legos.\n",
      "\n",
      "And Stumblewyk, who has been on both sides of the issue (having lost a child and then finally adopted a son) adds:\n",
      "\n",
      "I could have lived happily without a child. But I know for sure that I'll live HAPPIER with one. I'll be a much, much happier person doing everything in my power to make my son's life as perfect as I possibly can.\n",
      "\n",
      "…Or Hate Them (or Just Don't Love Them Enough to Want to Raise Them)\n",
      "\n",
      "Not everyone adores kids, though. On the opposite side of the spectrum, Platypus Man is frank about his feelings about kids:\n",
      "\n",
      "I hate kids. I've hated kids since I was a kid. As I've grown up (I'm 26 now), they can be barely ok in small doses once they're old enough to talk, but as a rule I still hate them. I give them evil stares in the grocery store when the parents aren't looking. I cringe when someone on Facebook posts pictures of their kids (expect to be unfollowed, likely forever, on picture number 2). I think that, without exception, all babies are ugly all the time.\n",
      "\n",
      "Even if you don't downright hate them, having kids is a lot of responsibility—one not everyone is ready for or willing to take on. Nonrg1 says:\n",
      "\n",
      "I myself love kids, but the problem is I don't like them enough to have some myself. I get the whole kids makes you happier thing but I don't want to be the one having to deal with changing diapers, and the whole puberty thing. I am not ready for that\n",
      "\n",
      "Craig Lloyd agrees:\n",
      "\n",
      "I get that kids are an amazing gift and there's nothing better than bringing life to this Earth, bla bla bla, but as it stands now, I'm just not interested in having kids at all. Maybe I'm just selfish, but I want to enjoy life and do things without having a huge obstacle in the way. […] Don't get me wrong, kids are cute and fun to play with, but as Michael Scott once said, \"Why be a dad when you can be a fun uncle? I've never heard of anyone rebelling against their fun uncle.\"\n",
      "\n",
      "You have to separate the idea of having children with actually wanting to have them, Wittyname says:\n",
      "\n",
      "I always assumed I would have kids. I do love children. I knew that someday I would have some of my own. But honestly, as I got into my 30s I realized that I was in love of the concept of having kids more than actually wanting to have them. Don't get me wrong; I adore children. But whenever I thought about kids it was always \"10 years from now in the distant future.\" When I started to think, would I want one now? next year? in 5 years? I would always mentally respond with \"hell no!\" It wasn't an easy thing to adjust to.\n",
      "\n",
      "The World Doesn't Need More People\n",
      "\n",
      "We have an over-population problem, which some may argue is the biggest problem the world faces. GulleKlein says we don't need to propagate anymore and it's selfish to create a new person when the world is already brimming with human beings:\n",
      "\n",
      "There are 7 billion people already. The species is doing OK. do we really need any more? When I hear people explain why they have or want to have kids it usually fits into two categories: they do it because they think they are supposed to (culture, religion, it's what their parents/friends/neighbors did), or some weird selfish desire (I love kids, teaching kids, watch them grow, want a \"family,\" etc).\n",
      "\n",
      "Instead of bearing children, you should adopt, aem2 adds:\n",
      "\n",
      "I think it's a tragedy that so many people have children when there are so many children in need of homes. [….] Too many parents, I feel, have children to fill some hole in their lives. If you are going to have children, make sure it is about them, not about you. And adopt.\n",
      "\n",
      "…But Maybe the World Could Use More Good People with Your Genes\n",
      "\n",
      "Assuming you can raise smart, responsible, and decent children, you may be doing the world a favor by becoming a parent, argues SnowmanPants:\n",
      "\n",
      "A lot of people say \"why would I want to bring a child into this shit hole of a world? We have so many problems, blah blah blah\" That's exactly why you should bring children into it, so you can raise them to be responsible, caring people who will fix the problems we face. I'm always saddened when my smart, caring friends say they're not having kids. They're depriving the world of countless smart, caring people who can improve things in the future.\n",
      "\n",
      "It could be a way of giving back to society, adds Patpcs1, who decided to have kids both out of Christian duty and a love of teaching:\n",
      "\n",
      "I also am an educator, and my wife is as well. We love the opportunity to teach, and have few things we love more than that aha moment when someone finally gets it. Parenting isn't an easy job, but you get to see those so many times, and if you think you're even half-way decent at teaching & parenting you hope that the kid(s) you raise will turn out at least a little better than average, both in terms of their own success and happiness, and their ability to give back to society.\n",
      "\n",
      "Do What's Right for You\n",
      "\n",
      "In the end, it comes down to what's right for you, and, as many commenters pointed out, it's not a decision anyone should make lightly (there's no turning back once you're a parent!). JTS says:\n",
      "\n",
      "I guess my only real advice is to really think hard about it. It seems like a lot of people have kids because that's just what people *do*. Whatever choice you make, do it intentionally. Think about your threshold for chaos, what you want your life to look like in 30-40 years, and (unfortunately this needs to be said) your economic ability to support a child, or multiple children. Don't have kids just to have them. Don't not have them just because you're afraid it's too hard.\n",
      "\n",
      "Perhaps a good litmus test would be to spend more time around more types of kids. Our own Whitson Gordon is starting to sway more towards having kids after spending time with some non-monstrous ones:\n",
      "\n",
      "I don't have kids yet, and I always thought I wanted them, but as I got a little older I wasn't sure. So many kids are like little monsters, and it sounds exhausting! I hadn't ruled it out, but I thought that maybe if I didn't have kids, I probably wouldn't be too sad about it. However, my fiancee's brothers both have kids, and they're pretty amazing. After spending time with her nieces, it's pretty hard for me to imagine a life in which I don't have kids—I'd just get so sad every time I saw these kids knowing I'd never have any of my own. Sure, they're a lot of work—I have no idea how her brothers/sisters-in-law do it!—but it seems totally worth it.\n",
      "\n",
      "Tom4Surfing has the final word:\n",
      "\n",
      "Choose the life you want for yourselves and then make it happen to the best of your ability.\n",
      "\n",
      "There's really no right or wrong answer—just careful, deliberate choices. \n",
      "------------------------------------\n",
      "label=family\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    random_choice = random.randint(0, len(X_test))\n",
    "    text, label = X_test[random_choice], y_test[random_choice]\n",
    "    print(text, '\\n------------------------------------')\n",
    "    print('label={}'.format(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['meteorology']\n"
     ]
    }
   ],
   "source": [
    "text_vectorized = vectorizer.transform([text])\n",
    "print(model.predict(text_vectorized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- persisting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./text_analysis_classifier_1587325013.joblib']"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "timestamp = int(time.time())\n",
    "# Save the vectorizer\n",
    "joblib.dump(vectorizer, './text_analysis_vectorizer_%s.joblib' % timestamp)\n",
    "# Save the classifier\n",
    "joblib.dump(model, './text_analysis_classifier_%s.joblib' % timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['meteorology']\n"
     ]
    }
   ],
   "source": [
    "# Load the vectorizer\n",
    "vectorizer = joblib.load('./text_analysis_vectorizer_%s.joblib' % timestamp) \n",
    "# Load the classifier\n",
    "classifier = joblib.load('./text_analysis_classifier_%s.joblib' % timestamp)\n",
    "# Test the loaded cobmponent by classifying a text\n",
    "print(classifier.predict(vectorizer.transform([text])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
